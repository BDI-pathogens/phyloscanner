---
title: "Robust Gaussian Processes in Stan"
author: "Michael Betancourt"
date: "October 2017"
output:
  html_document:
    fig_caption: yes
    theme: spacelab #sandstone #spacelab #flatly
    highlight: pygments
    toc: TRUE
    toc_depth: 2
    number_sections: TRUE
    toc_float:
      smooth_scroll: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment=NA)
```

Part 1: Introduction to Gaussian Processes
[Part 2: Optimizing Gaussian Processes Hyperparameters](../gp_part2/part2.html)
[Part 3: Bayesian Inference of Gaussian Processes Hyperparameters](../gp_part3/part3.html)

Often data decompose into _variates_, $y$, and _covariates_, $x$, with the
covariates being particularly straightforward to measure.  A _regression
model_ quantifies the statistical relationship between the variates and
covariates so that if new covariates are measured then the corresponding
variates can be inferred from the model.

Given the variate-covariate decomposition, the likelihood is more naturally
written as
$$ \pi (y, x \mid \theta, \phi)
=
\pi(y \mid x, \theta, \phi) \, \pi(x \mid \theta, \phi).$$
If we assume that the distribution of the covariates does not depend on
the parameters influencing the distribution of the variates,
$$ \pi (y, x \mid \theta, \phi)
= \pi(y \mid x, \theta) \, \pi(x \mid \phi),$$
then our regression model is completely quantifies in the conditional
likelihood $\pi(y \mid x, \theta)$.

In general a conditional likelihood can be constructed by first assuming
a model for the statistical variation in the variates,
$\pi(y \mid \theta_1, \ldots, \theta_N)$, and then replacing one or more
of the parameters with deterministic functions of the covariates, for
example, $\pi(y \mid f(x, \theta_1), \theta_2, \ldots, \theta_N)$.  For
example, we might model the measurement variation in the variates as
Gaussian with standard deviation $\sigma$ and mean given by a linear
function of the covariates, $\mu = \beta \cdot x + \alpha$.

This is a fine model if the data are consistent with a linear relationship,

```{r, comment=NA, echo=FALSE}
set.seed(689934)
x = sort(runif(10, -5, 5))
y = 2 * x + 1 + rnorm(10, 0, 2)
par(mar = c(4, 4, 0.5, 0.5))
plot(x, y, xlab="x", ylab="y", col="black", pch=16, cex=1)
```

but becomes much less satisfactory when there are more complex interactions
evident in the the data,

```{r, comment=NA, echo=FALSE}
x = sort(runif(10, -5, 5))
y = x * x * sin(x - 2) + 1 + rnorm(10, 0, 2)
par(mar = c(4, 4, 0.5, 0.5))
plot(x, y, xlab="x", ylab="y", col="black", pch=16, cex=1)
```

We can model more complex regression interactions by including a more complex
mean function, such as a high-order polynomial of the input covariates, but
these regression models quickly become ungainly to fit and often prone to
overfitting, where the model misinterprets the statistical variation in the
variates as deterministic behavior.

Bayesian methods are a particularly useful way to regularize overfitting,
but principled prior assignment on such polynomial expansions can be awkward.
Fortunately we can avoid parametric functions entirely and make inferences
over entire spaces of functions using _Gaussian processes_.

In this series of case studies we will learn about the very basics of Gaussian
processes, how to implement them in Stan, and how they can be robustly
incorporated into Bayesian models to avoid subtle but pathological behavior.

# Gaussian Processes

Formally Gaussian processes define probability distributions over
infinite-dimensional Hilbert spaces of functions.  A loose way of interpreting
these spaces is having each dimension correspond to the function output at one
of the infinite covariate inputs, $f(x)$.  The Gaussian process defines a
distribution over those infinite outputs.  In particular, if we sample from a
Gaussian process then we obtain an infinite number of point outputs that
stitch together to recover an entire function.

As with a finite-dimensional Gaussian distribution, Gaussian processes are
specified with a mean and covariance,
$$ f \sim \mathcal{GP}(m, k),$$
only these are now given by functions instead of vectors and matrices.

## Mean Functions

The mean function of a Gaussian process, $m(x)$, defines the base function
around which all of the realizations of the Gaussian process will be
distributed.  In other words, if we average an infinite number of realizations
of the Gaussian process then we recover the mean function.

Typically we don't have any strong information about the mean of our
regression model in which case we default to the zero function that returns
zero for all covariate inputs.  Even if we do have information about the
average function behavior, however, we can always decouple it from the
Gaussian process by linearity.  In other words, we will often use
Gaussian processes not to model the regression directly but rather to
model residual deviation around some parametric regression model.

In this case study we will assume a zero mean function.

## Covariance Functions

The covariance function or covariance kernel, $k(x_1, x_2)$ controls how
the realizations of a Gaussian process vary around the mean function.
The larger to covariance between two covariates, $x_1$ and $x_2$, the
less their functional outputs, $f(x_1)$ and $f(x_2)$, can vary.

There are many other covariance functions in common use, and those common
covariance functions can be combined to yield even more covariance functions.
Not only is the sum of two covariance functions also a covariance function,
so too is their product.  Consequently we can build quite sophisticated
Gaussian processes by exploiting just a few base covariance functions.

Perhaps the most common covariance function, and the one we will use
in this case study exclusively, is the _exponentiated quadratic_
kernel,
$$ k(x_1, x_2) =
\alpha^{2} \exp \left( - \frac{ (x_1 - x_2)^{2} }{\rho^{2}} \right).$$
Unfortunately naming conventions for this kernel are not entirely
consistent across fields or even time, and the reader might also find
it referred to as a squared exponential kernel, a quadratic exponential
kernel, a radial basis function or RBF kernel, and a Gaussian kernel,
amongst other names.

The exponentiated quadratic kernel supports smooth functions between
covariate inputs and a one-dimensional variate output.  The _marginal
standard deviation_, $\alpha$, controls the expected variation in
the variate output while the _length scale_, $\rho$, controls how
quickly the variate output varies with respect to the distance
between covariates.  In other words, the marginal standard deviation
controls how strong the functions wiggle and the length scale controls
who quickly they wiggle.

Keep in mind that the influences of these two hyperparameters are not
independent.  Even if the length scale is very long, for example, we
can increase the marginal standard deviation to specify a Gaussian
process with rapid wiggles.  Consequently care is needed when
interpreting the consequences of a given set of hyperparameters.

## Gaussian Processes in Practice

The concept of a Gaussian process is certainly appealing, but even
if the mathematics is well-posed the infinite-dimensional functions
supported by Gaussian processes will be too ungainly to manipulate
in practice.  Fortunately, Gaussian processes have an incredibly
useful _marginalization_ property that allows for the relevant
parts of those functions to be manipulated with finite computational
resources.

In any practical application we only ever consider a finite set of
covariates, for example the covariates in our observed data and the
covariates for which we want to model variate values.  Over such a
finite set of inputs a Gaussian process will always marginalizes to
a multivariate Gaussian distribution with mean components
$$\mu_{i} = m(x_{i})$$
and covariance matrix components
$$\Sigma_{ij} = k(x_{i}, x_{j}).$$
The marginalized covariance matrix is also known as the _Gram
matrix_ of the covariance kernel.

Consequently we can perform all of the manipulations of a Gaussian
processes that we need in practice using just the familiar properties
of multivariate Gaussian distributions.

# Gaussian Process Regression

Given a regression model such as
$$\pi(y \mid x, \sigma) = \mathcal{N} (y \mid f(x), \sigma)$$
or
$$\pi(y \mid x, \phi) = \mathrm{NegBin2}(y \mid f(x), \phi)$$
we introduce a Gaussian process as a prior over the functions $f(x)$
which defines a joint model over the statistical model for the variates
and its functional dependency on the covariates.  Given observations we
can then learn a Gaussian process posterior over those functional
dependencies most consistent with the data.

Conveniently, a Gaussian observation model is conjugate to a
Gaussian process prior.  In other words if
$$y \sim \mathcal{N} (f(x), \sigma)$$
$$f \sim \mathcal{GP}(m, k)$$
then the posterior is a Gaussian process with analytic
mean and covariance functions.  The posterior Gaussian process
also has mean function $m$ but the covariance function is modified
to
$$k'(x_1, x_2) = k(x_1, x_2) + \sigma^{2} \delta(x_1 - x_2),$$
where $\delta(0) = 1$ and vanishes for all other values.

Given observed variate-covariate pairs $\{y, x\}$ and lone
covariates $x'$ we can then construct a joint multivariate Gaussian
distribution over all of the covariates, and then condition on $y$
to construct the marginal posterior distribution for unobserved
variates $y'$ corresponding to the $x'$.

If the statistical model for the variates is not Gaussian then
we do not have such a clean analytic result.  Instead we have
to represent the Gaussian process as a latent multivariate
Gaussian distribution joint over the covariates with observed
and unobserved variates and approximate the posterior for
the unobserved variates, say with Markov chain Monte Carlo so
adequately provided by Stan.

# Gaussian Processes in Stan

With a very cursory introduction to Gaussian processes we are
now ready to consider their implementation in Stan.  As noted
above, once we have constructed the marginal mean vector and
covariance matrix their implementation reduces to common
Gaussian manipulations.

We'll begin by sampling from a Gaussian process prior and
then both analytic and non-analytic Gaussian process
posteriors.

## Simulating From A Gaussian Process Prior

Sampling a function from a Gaussian process prior, and then
simulating data corresponding to that function, is a
straightforward application of a multivariate Gaussian
random number generator.

First we take care of some preliminary setup,

```{r, comment=NA}
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
source("gp_utility.R")
```

and then define the Gaussian process hyperparameters, $\alpha$ and $\rho$,
the Gaussian measurement variation $\sigma$, and covariates at which we
will evaluate the Gaussian process.

```{r, comment=NA}
alpha_true <- 3
rho_true <- 5.5
sigma_true <- 2

N_total = 501
x_total <- 20 * (0:(N_total - 1)) / (N_total - 1) - 10

simu_data <- list(alpha=alpha_true, rho=rho_true, sigma=sigma_true,
                  N=N_total, x=x_total)
```

To sample from the Gaussian process we simply construct the marginal
covariance matrix, here using the builtin `cov_exp_quad` function,
and then simulate data by adding Gaussian variation around the
sampled function.  Note that we add a _nugget_ or _jitter_ of $10^{-10}$
so the marginal covariance matrix before computing its Cholesky
decomposition in order to stabilize the numerical calculations.  Often
the nugget is taking to be square root of the floating point precision,
which would be $10^{-8}$ for double precision calculations.

```{r, comment=NA}
writeLines(readLines("simu_gauss.stan"))
```

```{r, comment=NA}
simu_fit <- stan(file='simu_gauss.stan', data=simu_data, iter=1,
            chains=1, seed=494838, algorithm="Fixed_param")
```

From the 501 sampled points let's reserve 11 evenly spaced points
as observed data and leave the remaining 490 as held-out data.

```{r, comment=NA}
f_total <- extract(simu_fit)$f[1,]
y_total <- extract(simu_fit)$y[1,]

true_realization <- data.frame(f_total, x_total)
names(true_realization) <- c("f_total", "x_total")

observed_idx <- c(50*(0:10)+1)
N = length(observed_idx)
x <- x_total[observed_idx]
y <- y_total[observed_idx]

plot(x_total, f_total, type="l", lwd=2, xlab="x", ylab="y",
     xlim=c(-10, 10), ylim=c(-10, 10))
points(x_total, y_total, col="white", pch=16, cex=0.6)
points(x_total, y_total, col=c_mid_teal, pch=16, cex=0.4)
points(x, y, col="white", pch=16, cex=1.2)
points(x, y, col="black", pch=16, cex=0.8)
```

In general we will be interested in predictions over both the
observed data and the held-out data.

```{r, comment=NA}
N_predict <- N_total
x_predict <- x_total
y_predict <- y_total
```

Per good Stan workflow we save these simulated data in its own file.

```{r, comment=NA}
stan_rdump(c("N", "x", "y",
             "N_predict", "x_predict", "y_predict",
             "sample_idx"), file="gp.data.R")
data <- read_rdump("gp.data.R")

stan_rdump(c("f_total", "x_total", "sigma_true"), file="gp.truth.R")
```

For future comparison, we can also construct the prior data
generating process conditioned on the true realization of the
Gaussian process.

```{r, comment=NA}
f_data <- list(sigma=sigma_true, N=N_total, f=f_total)
dgp_fit <- stan(file='simu_gauss_dgp.stan', data=f_data, iter=1000, warmup=0,
                chains=1, seed=5838298, refresh=1000, algorithm="Fixed_param")

plot_gp_pred_quantiles(dgp_fit, data, true_realization,
                       "True Data Generating Process Quantiles")
```

The concentric intervals here cover probabilities of 20%, 40%, 60%,
and 80%.

## Simulating From a Gaussian Process Posterior with Gaussian Observations

Simulating from a Gaussian process posterior is straightforward
given the analytic manipulations available.  We simply use the
posterior kernel to construct a multivariate Gaussian distribution
joint over all of the covariates values and then analytically
condition on those covariates with observed variates.

```{r, comment=NA}
writeLines(readLines("predict_gauss.stan"))
```

```{r, comment=NA}
pred_data <- list(alpha=alpha_true, rho=rho_true, sigma=sigma_true, N=N, x=x, y=y,
                  N_predict=N_predict, x_predict=x_predict)
pred_fit <- stan(file='predict_gauss.stan', data=pred_data, iter=1000, warmup=0,
                     chains=1, seed=5838298, refresh=1000, algorithm="Fixed_param")
```

We can readily visualize posterior Gaussian
process, either with sampled realizations,

```{r, comment=NA}
plot_gp_realizations(pred_fit, data, true_realization,
                     "Posterior Realizations")
```

or quantiles of those realizations as a function of
the input covariate,

```{r, comment=NA}
plot_gp_quantiles(pred_fit, data, true_realization,
                  "Posterior Quantiles")
```

Similarly, we can visualize the posterior predictive
distribution that incorporates the Poisson measurement
variation with realizations,

```{r, comment=NA}
plot_gp_realizations(pred_fit, data, true_realization,
                     "Posterior Predictive Realizations")
```

or quantiles,

```{r, comment=NA}
plot_gp_pred_quantiles(pred_fit, data, true_realization,
                  "Posterior Predictive Quantiles")
```

## Simulating From a Gaussian Process Conditional on Non-Gaussian Observations

When the observation model is non-Gaussian the posterior Gaussian
process no longer has a closed form kernel.  In this case we have
to construct the multivariate Gaussian distribution joint over
all of the covariates within the model itself, and allow the fit
to explore the conditional realizations.

Consider, for example, a Poisson observation model where the Gaussian
process models the log rate,

```{r, comment=NA}
writeLines(readLines("simu_poisson.stan"))
```

```{r, comment=NA}
simu_fit <- stan(file='simu_poisson.stan', data=simu_data, iter=1,
            chains=1, seed=494838, algorithm="Fixed_param")
```

```{r, comment=NA}
f_total <- extract(simu_fit)$f[1,]
y_total <- extract(simu_fit)$y[1,]

true_realization <- data.frame(exp(f_total), x_total)
names(true_realization) <- c("f_total", "x_total")

sample_idx <- c(50*(0:10)+1)
N = length(sample_idx)
x <- x_total[sample_idx]
y <- y_total[sample_idx]

data = list("N"=N, "x"=x, "y"=y,
             "N_predict"=N_predict, "x_predict"=x_total, "y_predict"=y_total)

plot(x_total, exp(f_total), type="l", lwd=2, xlab="x", ylab="y",
     xlim=c(-10, 10), ylim=c(0, 10))
points(x_total, y_total, col="white", pch=16, cex=0.6)
points(x_total, y_total, col=c_mid_teal, pch=16, cex=0.4)
points(x, y, col="white", pch=16, cex=1.2)
points(x, y, col="black", pch=16, cex=0.8)
```

One way of implementing the model in Stan is to pull the covariates
with variate observations out of the vector of all of the covariates
in order to specify the observation model.  Because the model is no
longer conjugate we have to fit the latent Gaussian process with
Markov chain Monte Carlo in Stan,

```{r, comment=NA}
writeLines(readLines("predict_poisson.stan"))
```

Note that I have exploited the _non-centered parameterization_ of
the latent multivariate Gaussian which takes advantage of the fact
that
$$ \mathbf{f} \sim \mathcal{N} ( \boldsymbol{\mu}, \boldsymbol{\Sigma})$$
is equivalent to
$$ \tilde{\mathbf{f}} \sim \mathcal{N} (0, 1) $$
$$ \mathbf{f} = \boldsymbol{\mu} + \mathbf{L} \tilde{\mathbf{f}} $$
where
$$ \boldsymbol{\Sigma} = \mathbf{L} \mathbf{L}^{T} $$
Although these forms define the same distribution for $\mathbf{f}$,
the latter induces a nicer posterior geometry when the information
contained in the data is sparse.

```{r, comment=NA}
pred_data <- list(alpha=alpha_true, rho=rho_true,
                  N_predict=N_predict, x_predict=x_predict,
                  N_observed=N, y_observed=y, observed_idx=observed_idx)
pred_fit <- stan(file='predict_poisson.stan', data=pred_data, seed=5838298, refresh=1000)
```

As in the non-Gaussian observation case, once we have fit
the posterior Gaussian process we can visualize it with
sampled realizations

```{r, comment=NA}
plot_gp_realizations(pred_fit, data, true_realization,
                     "Posterior Realizations")
```

or quantiles of those realizations as a function of
the input covariate,

```{r, comment=NA}
plot_gp_quantiles(pred_fit, data, true_realization,
                  "Posterior Quantiles")
```

Similarly, we can visualize the posterior predictive
distribution that incorporates the Gaussian measurement
variation with realizations,

```{r, comment=NA}
plot_gp_realizations(pred_fit, data, true_realization,
                     "Posterior Predictive Realizations")
```

or quantiles,

```{r, comment=NA}
plot_gp_pred_quantiles(pred_fit, data, true_realization,
                  "Posterior Predictive Quantiles")
```

# Conclusion

Gaussian processes provide a flexible means of modeling non-parametric
regression behavior, both when the observation model is Gaussian and
non-Gaussian.  Moreover, given a choice of mean function and covariate
function the implementation of a Gaussian processes is straightforward.

But how exactly do we specify a covariance function?  Not only do we have
to choose a functional form, we also have to select the _hyperparameters_.
The specific value of these hyperparameters, however, can have a drastic
effect on the performance of the resulting Gaussian process.  In order
to ensure optimal performance we will have to _infer_ the hyperparameters
from the observed data.  Unfortunately that turns out to be no easy feat.

In Parts 2 and 3 of this case study we'll investigate how to infer
Gaussian process hyperparameters with maximum marginal likelihood
and Bayesian inference, paying particular attention to what can go
wrong and how we can maintain robust performance.

[Part 2: Optimizing Gaussian Processes Hyperparameters](../gp_part2/part2.html)
[Part 3: Bayesian Inference of Gaussian Processes Hyperparameters](../gp_part3/part3.html)

# Acknowledgements

The insights motivating this case study came from a particularly
fertile research project with Dan Simpson, Rob Trangucci, and
Aki Vehtari.

I thank Dan Simpson, Aki Vehtari, and Rob Trangucci for many
helpful comments on the case study.

# Original Computing Environment

```{r, comment=NA}
writeLines(readLines(file.path(Sys.getenv("HOME"), ".R/Makevars")))
```

```{r, comment=NA}
devtools::session_info("rstan")
```
